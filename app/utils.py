import pandas as pd
import numpy as np
import time
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_squared_error
from typing import Tuple, Optional, Any, Dict

# =========================================================
# ğŸ”’ Forecast Guardrail
# =========================================================
def apply_forecast_guardrail(
    forecast_values: np.ndarray | list,
    historical_series: pd.Series,
    max_daily_growth: float = 1.15,
    ceiling_multiplier: float = 1.25,
    rolling_window: int = 3
) -> np.ndarray:
    """
    ØªØ·Ø¨ÙŠÙ‚ Ù‚ÙŠÙˆØ¯ Ø°ÙƒÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ù„Ù…Ù†Ø¹ Ø§Ù„Ù†Ù…Ùˆ Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø±ÙŠ.
    
    Args:
        forecast_values: Ù‚Ø§Ø¦Ù…Ø© Ø£Ùˆ Ù…ØµÙÙˆÙØ© numpy Ù„Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ©.
        historical_series: Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ù„Ù…Ø¨ÙŠØ¹Ø§Øª.
        max_daily_growth: Ø£Ù‚ØµÙ‰ Ù†Ø³Ø¨Ø© Ù†Ù…Ùˆ ÙŠÙˆÙ…ÙŠØ© Ù…Ø³Ù…ÙˆØ­ Ø¨Ù‡Ø§.
        ceiling_multiplier: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¹Ù„Ù‰ (Ù†Ø³Ø¨Ø© Ù…Ù† Ø£Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ø© ØªØ§Ø±ÙŠØ®ÙŠØ©).
        rolling_window: Ø­Ø¬Ù… Ø§Ù„Ù†Ø§ÙØ°Ø© Ù„ØªÙ†Ø¹ÙŠÙ… Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØªØ­Ø±Ùƒ.
    
    Returns:
        np.ndarray: ØªÙˆÙ‚Ø¹Ø§Øª Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù€ guardrail ÙˆØ§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØªØ­Ø±Ùƒ.
    """
    forecast = pd.Series(forecast_values).copy()
    hist_max = historical_series.max()
    ceiling = hist_max * ceiling_multiplier

    for i in range(1, len(forecast)):
        allowed = forecast[i - 1] * max_daily_growth
        forecast[i] = min(forecast[i], allowed)

    forecast = np.minimum(forecast, ceiling)
    forecast = forecast.rolling(window=rolling_window, min_periods=1).mean()

    return forecast.values


# =========================================================
# ğŸš€ Forecast Generation
# =========================================================
def generate_forecast(
    df: pd.DataFrame,
    horizon: int,
    multiplier: float = 1.0,
    residuals_std: Optional[float] = None,
    use_guardrail: bool = True
) -> Tuple[np.ndarray, Any, Any, Any]:
    """
    ØªÙˆÙ„ÙŠØ¯ ØªÙˆÙ‚Ø¹Ø§Øª Ù…Ø¨ÙŠØ¹Ø§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù…Ø¹ Ø¯Ø¹Ù… guardrail.
    
    Args:
        df: DataFrame ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ 'sales'.
        horizon: Ø¹Ø¯Ø¯ Ø£ÙŠØ§Ù… Ø§Ù„ØªÙˆÙ‚Ø¹ Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©.
        multiplier: Ù…Ø¹Ø¯Ù„ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ (Ù…ØªÙØ§Ø¦Ù„ØŒ ÙˆØ§Ù‚Ø¹ÙŠØŒ Ù…ØªØ´Ø§Ø¦Ù…).
        residuals_std: Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ Ù„Ù„Ù…Ø®Ù„ÙØ§Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ).
        use_guardrail: ØªÙØ¹ÙŠÙ„ Guardrail Ù„Ù…Ù†Ø¹ Ø§Ù„Ù†Ù…Ùˆ Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø±ÙŠ.
    
    Returns:
        tuple: (forecast_array, None, None, None) Ù„ØªÙˆØ§ÙÙ‚ Ø§Ù„Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯.
    """
    if 'sales' not in df.columns:
        raise ValueError("DataFrame ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ 'sales'.")

    historical_max = df['sales'].max()
    prev_value = df['sales'].iloc[-1]
    predictions = []

    for step in range(horizon):
        raw_pred = prev_value * 1.05 * multiplier

        if use_guardrail:
            allowed_growth = prev_value * 1.15
            ceiling = historical_max * 1.25
            safe_pred = min(raw_pred, allowed_growth, ceiling)
        else:
            safe_pred = raw_pred

        predictions.append(safe_pred)
        prev_value = safe_pred

    final_pred = pd.Series(predictions).rolling(3, min_periods=1).mean().values
    return final_pred, None, None, None


# =========================================================
# ğŸ“Š Backtesting
# =========================================================
def run_backtesting(
    df: pd.DataFrame,
    feature_names: list,
    scaler: Any,
    model: Any,
    num_cols: Optional[list] = None,
    n_splits: int = 3
) -> Dict[str, Any]:
    """
    Ø¥Ø¬Ø±Ø§Ø¡ Backtesting Ø¹Ù„Ù‰ Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª.
    
    Args:
        df: DataFrame ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª.
        feature_names: Ù‚Ø§Ø¦Ù…Ø© Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤.
        scaler: ÙƒØ§Ø¦Ù† Scaler Ù„ØªØ·Ø¨ÙŠÙ‚Ù‡ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ©.
        model: Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø¯Ø±Ø¨ (CatBoost Ø£Ùˆ ØºÙŠØ±Ù‡).
        num_cols: Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ© Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ scaling.
        n_splits: Ø¹Ø¯Ø¯ ØªÙ‚Ø³ÙŠÙ…Ø§Øª TimeSeries.
    
    Returns:
        dict: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ metrics Ù…Ø«Ù„ MAPE, RMSE, R2, residuals_std, execution_time, data_points, features_count
    """
    start_time = time.time()
    df = df.copy().replace([np.inf, -np.inf], np.nan).dropna(subset=['sales'])
    results = []
    all_residuals = []

    if not num_cols:
        num_cols = ['lag_1', 'lag_7', 'rolling_mean_7', 'rolling_mean_14']

    if len(df) < (n_splits + 1) * 7:
        n_splits = max(2, n_splits - 1)

    tscv = TimeSeriesSplit(n_splits=n_splits)

    for train_idx, test_idx in tscv.split(df):
        test_df = df.iloc[test_idx].copy()
        X_test = test_df[feature_names].copy().ffill().bfill().fillna(0)

        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù€ Scaler
        try:
            X_test[num_cols] = scaler.transform(X_test[num_cols])
        except:
            X_test_scaled = scaler.transform(X_test)
            X_test = pd.DataFrame(X_test_scaled, columns=feature_names, index=X_test.index)

        # ØªÙˆÙ‚Ø¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
        preds_log = model.predict(X_test)
        preds = np.expm1(preds_log)
        actuals = test_df['sales'].values
        preds = np.nan_to_num(preds, nan=0.0, posinf=actuals.max()*2, neginf=0.0)
        preds = np.maximum(preds, 0)
        residuals = actuals - preds
        all_residuals.extend(residuals)

        try:
            results.append({
                'mape': mean_absolute_percentage_error(actuals, preds),
                'rmse': np.sqrt(mean_squared_error(actuals, preds)),
                'r2': r2_score(np.log1p(actuals), preds_log)
            })
        except:
            continue

    if not results:
        return {
            'mape': 0.1, 'rmse': 0, 'r2': 0,
            'residuals_std': 1.0, 'execution_time': 0,
            'data_points': len(df), 'features_count': len(feature_names)
        }

    metrics_avg = pd.DataFrame(results).mean().to_dict()
    metrics_avg['residuals_std'] = np.std(all_residuals) if all_residuals else 1.0
    metrics_avg['execution_time'] = time.time() - start_time
    metrics_avg['data_points'] = len(df)
    metrics_avg['features_count'] = len(feature_names)

    return metrics_avg
