import pandas as pd
import numpy as np
import streamlit as st
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_squared_error
import time

@st.cache_data(show_spinner=False)
def run_backtesting(_df, feature_names, _scaler, _model):
    """
    Ù†Ø³Ø®Ø© Backtesting Ø§Ù„Ù…Ø·ÙˆØ±Ø©:
    - ØªØ¹Ø§Ù„Ø¬ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù€ inf% ÙÙŠ Ø§Ù„Ù€ MAPE.
    - ØªØ­Ù…ÙŠ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ØµÙØ±ÙŠØ© ÙˆØ§Ù„Ù€ Outliers.
    """
    start_time = time.time()
    
    # 1. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    _df = _df.copy().replace([np.inf, -np.inf], np.nan)
    _df['sales'] = _df['sales'].fillna(0)
    
    tscv = TimeSeriesSplit(n_splits=3)
    results = []
    all_residuals = []
    num_cols = ['lag_1', 'lag_7', 'rolling_mean_7', 'rolling_mean_14']
    
    for train_index, test_index in tscv.split(_df):
        test_df = _df.iloc[test_index].copy()
        X_test = test_df[feature_names].copy()
        
        # Ù…Ù„Ø¡ Ø§Ù„ÙØ±Ø§ØºØ§Øª ÙÙŠ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        X_test = X_test.ffill().bfill().fillna(0)
        
        # Scaling Ø¢Ù…Ù†
        try:
            X_test[num_cols] = _scaler.transform(X_test[num_cols])
        except:
            X_test_scaled = _scaler.transform(X_test)
            X_test = pd.DataFrame(X_test_scaled, columns=feature_names, index=X_df.index)
        
        # Ø§Ù„ØªÙˆÙ‚Ø¹
        preds_log = _model.predict(X_test)
        preds = np.expm1(preds_log)
        actuals = test_df['sales'].values
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª (Ù…Ù†Ø¹ Ø§Ù„Ø³Ø§Ù„Ø¨ ÙˆØ§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø®Ø±Ø§ÙÙŠØ©)
        preds = np.nan_to_num(preds, nan=0.0, posinf=actuals.max()*2)
        preds = np.maximum(preds, 0)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ Ù„Ù„Ù€ Confidence Interval
        residuals = actuals - preds
        all_residuals.extend(residuals)
        
        # ğŸ›¡ï¸ Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ (ØªØ¬Ù†Ø¨ Ø§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ ØµÙØ±)
        try:
            # Ù†Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„Ø£ÙŠØ§Ù… Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ Ù…Ø¨ÙŠØ¹Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© > 0 Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù€ MAPE
            mask = actuals > 0
            if np.any(mask):
                mape_val = mean_absolute_percentage_error(actuals[mask], preds[mask])
            else:
                mape_val = 0.0 # Ù„Ùˆ ÙƒÙ„ Ø§Ù„Ø£ÙŠØ§Ù… Ø£ØµÙØ§Ø±ØŒ Ø§Ù„Ù€ Error ØµÙØ±
                
            results.append({
                'mape': mape_val,
                'rmse': np.sqrt(mean_squared_error(actuals, preds)),
                'r2': r2_score(np.log1p(actuals), preds_log)
            })
        except:
            continue
            
    # Ù„Ùˆ Ù…ÙÙŠØ´ Ù†ØªØ§Ø¦Ø¬ (Ø­Ù…Ø§ÙŠØ© Ù†Ù‡Ø§Ø¦ÙŠØ©)
    if not results:
        return {'mape': 0.0, 'rmse': 0.0, 'r2': 0.0, 'residuals_std': 1.0, 
                'execution_time': 0, 'data_points': len(_df)}

    # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª
    metrics_avg = pd.DataFrame(results).mean().to_dict()
    
    # Ø­Ù…Ø§ÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ù€ R2 Ù„Ùˆ Ø·Ù„Ø¹ Ø±Ù‚Ù… Ø³Ø§Ù„Ø¨ Ø®ÙŠØ§Ù„ÙŠ
    metrics_avg['r2'] = max(metrics_avg.get('r2', 0), 0)
    
    metrics_avg['residuals_std'] = np.std(all_residuals) if all_residuals else 1.0
    metrics_avg['execution_time'] = time.time() - start_time
    metrics_avg['data_points'] = len(_df)
    
    return metrics_avg
